Decision Trees 

A Decision Tree is an algorithm for supervised learning and can be used in classification as well as regression tasks.
The basic principle is to learn simple decision rules from a labeled feature data set in order to predict a target variable. [1]

The decision tree is a tree data structure which contains nodes that correspond to individual features. The branches from a node to its children represent tests on the feature of the node. Each node therefore has as many children as there are possible values for the corresponding feature.
The leaf nodes represent predicted target values.

Predicting the target value for an unlabeled feature set can be accomplished by starting at the root node of the tree and following the edges which positively test for a feature given by the current node in the feature set to be classified, until a leaf node has been reached.
The time complexity is therefore linear in the height of the tree, i.e. O(h).

In the case of binary classification, a decision tree can be interpreted as a disjunction of conjunction of constraints [2], where each path from the root to a leaf corresponds to a conjunction of attribute tests.

Decision Trees have been successfully applied to many different problems, such as classifying medical patients by their disease, equipment malfunctions by their cause, and loan applicants by their likelihood of defaulting on payments. [2]

Decision Tree Learning

Since designing the structure of the tree and thereby the decision rules manually is too inefficient and likely suboptimal, decision tree learning algorithms are used to automatically construct a decision tree in a supervised fashion.

Most of these decision tree learning algorithms employ a top-down, greedy search through the space of possible decision trees. [2]
An example for this is the ID3 algorithm, which will be described in the next section.


ID3

The ID3 algorithm begins constructing a decision tree by starting at the root and selecting the "best" attribute to split the training data on. The specifics of what represents the "best" attribute will be described in a later section, but in general, each attribute is evaluated on its ability to classify training examples on its own.
This selected attributed is then used as the test at the root node, and for each possible value of the attribute, a child node for the root is created. The training data is split using the selected attribute and sorted to the child nodes correspondingly. [2]
This process is repeated recursively at each node, until all examples are perfectly classified or a depth threshold is met.

The algorithm forms a greedy search through the space of possible decision trees, as on each split a locally optimal decision is made, however without considering different choices for ancestor nodes.

// TODO: Add ID3 pseudocode here




[1]: http://scikit-learn.org/stable/modules/tree.html#tree
[2]: http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf